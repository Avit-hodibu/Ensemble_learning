{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnpWtbrErCkl80JexamgbB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avit-hodibu/Ensemble_learning/blob/main/Ensemble_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble learning\n",
        "collection of machine learning models.\n"
      ],
      "metadata": {
        "id": "GTVD3fV6Owdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wisdom of the crowd**\n",
        "Answer given by crowd is correct.\n"
      ],
      "metadata": {
        "id": "-NxsFZ7AYHpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction:\n",
        "we have different base ML(any models like decision tree, regression, SVM etc) algo models. Fhese collective models is called ensemble model. These model need to different by data or algo.\n",
        "\n",
        "In classification problem, if more model is saying it belong to some class than other. Then majority answer come from model is pick as correct.\n",
        "\n",
        "In regression problem, the output come from each model is taken mean. This mean is answer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ssWgNaOcZPLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Ensemble learning**\n",
        "1. Voting ensemble: Base model are different algorithm(like, model 1= SVM, model 2 = LoR, model 3 = DT ) and same data set. We give same queries to all models. And take the majority count.\n",
        "\n",
        "2. Bagging(Random Forest): Bootstrapped aggregation. same algorithm but have different dataset.\n",
        "\n",
        "  For eg: we have 1000 dataset and make 3 model of same algo (SVM). we randomly take 500 dataset and put in model1 and another 500 randomly in another model2 and another 500 in model3. We take the mean of it.\n",
        "\n",
        "  Random forest is especial case of bagging where Decision Tree is use. That is why name is forest a collection of trees\n",
        "\n",
        "3. Boosting(Adaboosting, Gradient boosting, XgBoosting): Most powerful technique. We take same model algo. we put data on model1 and it will notedown what are the mistakes. It tell all the mistake to the model that is after him and give data too . So, it can improve from mistake. It will do same note the mistake and give it to another model. It will again improve from mistake. It work on series and improve the mistake.\n",
        "\n",
        "4. Stacking: Base model are different algorithm(like, model 1= SVM, model 2 = LoR, model 3 = DT ) and same data set.  There is another model below that base model which will in herite all the answer of the base models and assign weight according to there correctness prediction. more the base model predict correct have the high weight than other. We give same queries to all models."
      ],
      "metadata": {
        "id": "g3wPluXhb1aD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AD:\n",
        "1. improvement in performance\n",
        "2. low bias and low variance\n",
        "3. robustness\n",
        "\n",
        "Dis:\n",
        "1. computational complexity increase"
      ],
      "metadata": {
        "id": "TmmCxPmkjl_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to use = always"
      ],
      "metadata": {
        "id": "Omvb8nz2k8uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting ensemble\n"
      ],
      "metadata": {
        "id": "Ypn8p5GFlKiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our models are train with same data. When the system is train the model is ready for prediction. Now, we call new queries data . We send Yq to all model. classification(The answer that comes from all models is counted. And we pick the majority.) regression (we take a mean of the answer)\n",
        "\n",
        "Assumption:\n",
        "1. All models should be independent to perform better. If the model are similar it wont perform good.\n",
        "2. all models accuray should be more than 50%"
      ],
      "metadata": {
        "id": "g65m5ZlIq9j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we created strong model using weak models(model have accuracy aleast 51%)?\n",
        "\n",
        "-> Accuracy increase as added independent model by little.\n",
        "\n",
        "if accuray less than 50 then adding more model will decrease it's accuracy rather than increaseing.\n"
      ],
      "metadata": {
        "id": "P6qGFiVluKfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Voting Classifier:\n",
        "\n",
        "We count majority from the models as the answer.\n"
      ],
      "metadata": {
        "id": "E51xdEGI0zkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voting type:\n",
        "\n",
        "1. Hard voting:\n",
        "if in model1,  0 has 0.6 prob and 1 has 0.4 then we see model1 predict 0. and in model2, 0 has prob as 0.8 and 1 has 0.2 then we see model2 perdict 0. So, overll will be 0.\n",
        "\n",
        "2. Soft voting:\n",
        "if in model1,  0 has 0.6 prob and 1 has 0.4 and in model2, 0 has prob as 0.8 and 1 has 0.2 then we will add 0 probability and 1 probablity of all model, model1 and model2.\n",
        "\n",
        "  0 proba = (0.6 + 0.8)/2 = 0.7\n",
        "\n",
        "  1 proba = (0.4 + 0.2)/2 = 0.3\n",
        "\n",
        "  then only since 0 has high prob so 0 is answer.\n",
        "\n",
        "Most of the time soft voting is better than hard voting."
      ],
      "metadata": {
        "id": "1uX0hTlH1hFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OdgfMQDMA7w"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#all models\n",
        "clf1 = LogisticRegression()\n",
        "clf2 = RandomForestClassifier()\n",
        "clf3 = KNeighborsClassifier()\n",
        "\n",
        "#model\n",
        "estimators = [('lr',clf1),('rf',clf2),('knn',clf3)]\n",
        "\n",
        "#voting classifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "#hard\n",
        "vc = VotingClassifier(estimators=estimators,voting='hard')\n",
        "x = cross_val_score(vc,X,y,cv=10,scoring='accuracy')\n",
        "print(np.round(np.mean(x),2))\n",
        "\n",
        "#soft\n",
        "vc1 = VotingClassifier(estimators=estimators,voting='soft')\n",
        "x = cross_val_score(vc1,X,y,cv=10,scoring='accuracy')\n",
        "print(np.round(np.mean(x),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight voting:\n",
        "```\n",
        "# model1 = 1 , model2= 2 and model3 =3 weight\n",
        "VotingClassifier(estimators=estimators,voting='soft', weights=[1, 2, 3])\n",
        "```"
      ],
      "metadata": {
        "id": "6DxpC2k2DUdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
        "\n",
        "svm1 = SVC(probability=True, kernel='poly', degree=1)\n",
        "svm2 = SVC(probability=True, kernel='poly', degree=2)\n",
        "svm3 = SVC(probability=True, kernel='poly', degree=3)\n",
        "svm4 = SVC(probability=True, kernel='poly', degree=4)\n",
        "svm5 = SVC(probability=True, kernel='poly', degree=5)\n",
        "\n",
        "estimators = [('svm1',svm1),('svm2',svm2),('svm3',svm3),('svm4',svm4),('svm5',svm5)]\n",
        "\n",
        "for estimator in estimators:\n",
        "    x = cross_val_score(estimator[1],X,y,cv=10,scoring='accuracy')\n",
        "    print(estimator[0],np.round(np.mean(x),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3cqBdIVEw5j",
        "outputId": "06499b52-6bcb-4f5b-9664-c0bdbbc7c4b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "svm1 0.85\n",
            "svm2 0.85\n",
            "svm3 0.89\n",
            "svm4 0.81\n",
            "svm5 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vc1 = VotingClassifier(estimators=estimators,voting='soft')\n",
        "x = cross_val_score(vc1,X,y,cv=10,scoring='accuracy')\n",
        "print(np.round(np.mean(x),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1T-o58XE-op",
        "outputId": "4d9f02f6-eae7-4ded-99f3-88e1cd35ec7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see it is performing better."
      ],
      "metadata": {
        "id": "pDkkWclYFb4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Voting Regression:\n"
      ],
      "metadata": {
        "id": "o26g_c3hFiuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will take a mean of answers from model and the mean will be answer."
      ],
      "metadata": {
        "id": "U0uNQMCmF1d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vr = VotingRegressor(estimators)\n",
        "scores = cross_val_score(vr,X,y,scoring='r2',cv=10)\n",
        "print(\"Voting Regressor\",np.round(np.mean(scores),2))"
      ],
      "metadata": {
        "id": "-3B0XBdCI5wn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}